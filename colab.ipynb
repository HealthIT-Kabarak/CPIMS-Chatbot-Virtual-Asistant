{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading our JSON Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kzCdBCZO8uZl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "import json\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracting Data\n",
        "- setup some blank lists to store these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kDJ4Uqpe874z"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " loop through our JSON data and extract the data we want. \n",
        " For each pattern we will turn it into a list of words using nltk.word_tokenizer, rather than having them as strings.\n",
        "add each pattern into  docs_x list and its associated tag into the docs_y list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ovVu1Clz886y"
      },
      "outputs": [],
      "source": [
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        words.extend(wrds)\n",
        "        docs_x.append(wrds)\n",
        "        docs_y.append(intent[\"tag\"])\n",
        "        \n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yc4uVlEK9Cxa"
      },
      "outputs": [],
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "labels = sorted(labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bag of Words\n",
        "\n",
        " If the position in the list is a 1 then that will mean that the word exists in our sentence, if it is a 0 then the word is not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pYPGfEx79Fkr"
      },
      "outputs": [],
      "source": [
        "training = []\n",
        "output = []\n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "\n",
        "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "    for w in words:\n",
        "        if w in wrds:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "convert our training data and output to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4yvu53d59KUn"
      },
      "outputs": [],
      "source": [
        "training = numpy.array(training)\n",
        "output = numpy.array(output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Developing a Model\n",
        "-fairly standard feed-forward neural network with two hidden layers.\n",
        "-give a class that they belong too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5VUejAES9Qxp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Shemm\\Desktop\\Lastminute\\Lastminute\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training & Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2Il752HA9R3s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.20864\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 1000 | loss: 0.20864 - acc: 0.9749 -- iter: 16/20\n",
            "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.17951\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 1000 | loss: 0.17951 - acc: 0.9774 -- iter: 20/20\n",
            "--\n",
            "INFO:tensorflow:c:\\Users\\Shemm\\Desktop\\Lastminute\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ],
      "source": [
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xVLOQ0xl9U6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start talking with the bot (type quit to stop)!\n",
            "Good to see you again\n"
          ]
        }
      ],
      "source": [
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "            \n",
        "    return numpy.array(bag)\n",
        "\n",
        "\n",
        "def chat():\n",
        "    print(\"Start talking with the bot (type quit to stop)!\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        results = model.predict([bag_of_words(inp, words)])\n",
        "        results_index = numpy.argmax(results)\n",
        "        tag = labels[results_index]\n",
        "\n",
        "        for tg in data[\"intents\"]:\n",
        "            if tg['tag'] == tag:\n",
        "                responses = tg['responses']\n",
        "\n",
        "        print(random.choice(responses))\n",
        "\n",
        "chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Lastminute",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "0292e43935a37f3fcd4062df416325198d2cbe4c24ce7ecec2731a75cfe99a75"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
